{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "om4ittXxN-O3",
        "outputId": "029cdd82-58b1-4905-d384-90c04c0c6321"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastdtw"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_XVB1lsvdJl",
        "outputId": "09ae3a10-7cfc-4f3a-cd2f-138986e21da7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fastdtw\n",
            "  Downloading fastdtw-0.3.4.tar.gz (133 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/133.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m133.1/133.4 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fastdtw) (2.0.2)\n",
            "Building wheels for collected packages: fastdtw\n",
            "  Building wheel for fastdtw (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fastdtw: filename=fastdtw-0.3.4-cp311-cp311-linux_x86_64.whl size=542088 sha256=56e4e749bfd815de582b4fd0ed630107a14419943978c1946df637f946a74375\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/8a/f6/fd3df9a9714677410a5ccbf3ca519e66db4a54a1c46ea95332\n",
            "Successfully built fastdtw\n",
            "Installing collected packages: fastdtw\n",
            "Successfully installed fastdtw-0.3.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dfW7fkstmalu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, recall_score, confusion_matrix, roc_curve, auc\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "from imblearn.metrics import specificity_score # specificity_score 임포트\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Conv1D, Flatten\n",
        "import seaborn as sns\n",
        "from fastdtw import fastdtw\n",
        "from scipy.spatial.distance import euclidean\n",
        "import random\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "PBqLiRHQmfcM"
      },
      "outputs": [],
      "source": [
        "df_X_train = pd.read_csv('/content/drive/My Drive/PhalangesOutlinesCorrect/X_train_Worms.csv')\n",
        "df_y_train = pd.read_csv('/content/drive/My Drive/PhalangesOutlinesCorrect/y_train_Worms.csv')\n",
        "df_X_test = pd.read_csv('/content/drive/MyDrive/PhalangesOutlinesCorrect/X_test_Worms.csv')\n",
        "df_y_test = pd.read_csv('/content/drive/MyDrive/PhalangesOutlinesCorrect/y_test_Worms.csv')\n",
        "X_train = df_X_train.values\n",
        "y_train = df_y_train.values.reshape(-1)  # 1차원 배열(벡터)로 변환\n",
        "X_test = df_X_test.values\n",
        "y_test = df_y_test.values.reshape(-1)  # 1차원 배열(벡터)로 변환\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1~5. 클래스별 증강이 필요한 개수 계산 및 목표 수 설정\n",
        "from collections import defaultdict\n",
        "\n",
        "class_counts = pd.Series(y_train).value_counts()\n",
        "total_samples = len(X_train)\n",
        "target_total = total_samples * 2\n",
        "avg_target = target_total / len(class_counts)\n",
        "\n",
        "non_aug_classes = {}\n",
        "aug_classes = {}\n",
        "for cls, count in class_counts.items():\n",
        "    if count >= avg_target:\n",
        "        non_aug_classes[cls] = count\n",
        "    else:\n",
        "        aug_classes[cls] = count\n",
        "\n",
        "remaining_target = target_total - sum(non_aug_classes.values())\n",
        "class_targets = {cls: int(remaining_target / len(aug_classes)) for cls in aug_classes}\n"
      ],
      "metadata": {
        "id": "_-mSz9Z7Lk0L"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "remaining_target"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZZh1vHnLmpY",
        "outputId": "de985fbf-4dcf-4ecc-db97-ee39bbd06d91"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "286"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_targets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OveJSRsyLnyR",
        "outputId": "adfd1871-2068-4684-f92e-ed9f870f3bf9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{4: 71, 2: 71, 3: 71, 5: 71}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6~9. 증강 수행 (DTW 기반 이웃 찾기 및 선형 보간)\n",
        "def find_k_neighbors_dtw(X_class, k):\n",
        "    neighbors_dict = {}\n",
        "    for i, A in enumerate(X_class):\n",
        "        distances = []\n",
        "        for j, B in enumerate(X_class):\n",
        "            if i == j:\n",
        "                continue\n",
        "            dist, _ = fastdtw(A.flatten(), B.flatten())\n",
        "            distances.append((j, dist))\n",
        "        distances.sort(key=lambda x: x[1])\n",
        "        neighbors_dict[i] = [idx for idx, _ in distances[:k]]\n",
        "    return neighbors_dict\n"
      ],
      "metadata": {
        "id": "zNyqx9HBLps-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def linear_interpolation(A, B, alpha):\n",
        "    return alpha * A + (1 - alpha) * B\n",
        "\n",
        "X_aug_list, y_aug_list = [], []\n",
        "\n",
        "for cls in aug_classes:\n",
        "    X_class = X_train[y_train == cls]\n",
        "    target = class_targets[cls]\n",
        "    n_to_generate = target - len(X_class)\n",
        "    if n_to_generate <= 0:\n",
        "        continue\n",
        "\n",
        "    neighbor_dict = find_k_neighbors_dtw(X_class, k=5)\n",
        "\n",
        "    for _ in range(n_to_generate):\n",
        "        idx_a = np.random.randint(0, len(X_class))\n",
        "        A = X_class[idx_a]\n",
        "        neighbors = neighbor_dict[idx_a]\n",
        "        idx_b = np.random.choice(neighbors)\n",
        "        B = X_class[idx_b]\n",
        "\n",
        "        alpha = np.random.uniform(0.2, 0.8)\n",
        "        S = linear_interpolation(A, B, alpha)\n",
        "        X_aug_list.append(S)\n",
        "        y_aug_list.append(cls)"
      ],
      "metadata": {
        "id": "GupFt6DqLsyP"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 최종 데이터 생성\n",
        "X_synthetic = np.vstack([X_train] + X_aug_list)\n",
        "y_synthetic = np.concatenate([y_train, y_aug_list])"
      ],
      "metadata": {
        "id": "K8HUhMaoL_7h"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"==== 증강 제외 클래스 ====\")\n",
        "for cls, count in non_aug_classes.items():\n",
        "    print(f\"Class {cls}: {count}개\")\n",
        "\n",
        "print(\"\\n==== 증강 대상 클래스 및 목표 샘플 수 ====\")\n",
        "for cls in aug_classes:\n",
        "    print(f\"Class {cls}: 기존 {aug_classes[cls]}개 → 목표 {class_targets[cls]}개 → 생성 {class_targets[cls] - aug_classes[cls]}개\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ivFdlhCL5PA",
        "outputId": "7a1d002d-ca41-4076-b9be-803550a362e0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==== 증강 제외 클래스 ====\n",
            "Class 1: 76개\n",
            "\n",
            "==== 증강 대상 클래스 및 목표 샘플 수 ====\n",
            "Class 4: 기존 32개 → 목표 71개 → 생성 39개\n",
            "Class 2: 기존 31개 → 목표 71개 → 생성 40개\n",
            "Class 3: 기존 25개 → 목표 71개 → 생성 46개\n",
            "Class 5: 기존 17개 → 목표 71개 → 생성 54개\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Label 인코딩\n",
        "le = LabelEncoder()\n",
        "y_synthetic_encoded = le.fit_transform(y_synthetic)\n",
        "y_test_encoded = le.transform(y_test)\n",
        "num_classes = len(np.unique(y_synthetic_encoded))\n",
        "\n",
        "# One-hot encoding\n",
        "y_synthetic_cat = to_categorical(y_synthetic_encoded, num_classes=num_classes)\n",
        "y_test_cat = to_categorical(y_test_encoded, num_classes=num_classes)\n"
      ],
      "metadata": {
        "id": "K4l0fEk0MywF"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_with_logistic_regression(X_train, y_train, X_test):\n",
        "    model = LogisticRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "    return model.predict(X_test)\n",
        "\n",
        "def predict_with_cart(X_train, y_train, X_test):\n",
        "    model = DecisionTreeClassifier()\n",
        "    model.fit(X_train, y_train)\n",
        "    return model.predict(X_test)\n",
        "\n",
        "def predict_with_knn(X_train, y_train, X_test, k=3):\n",
        "    model = KNeighborsClassifier(n_neighbors=k)\n",
        "    model.fit(X_train, y_train)\n",
        "    return model.predict(X_test)\n",
        "\n",
        "def predict_with_xgboost(X_train, y_train, X_test, label_encoder=None):\n",
        "    from xgboost import XGBClassifier\n",
        "\n",
        "    model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
        "    model.fit(X_train, y_train)\n",
        "    pred_y = model.predict(X_test)\n",
        "\n",
        "    if label_encoder is not None:\n",
        "        pred_y = label_encoder.inverse_transform(pred_y)\n",
        "\n",
        "    return pred_y\n",
        "\n",
        "def predict_with_lstm(X_train, y_train, X_test, num_classes):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(50, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "    X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "    model.fit(X_train_reshaped, y_train, epochs=50, batch_size=32, verbose=0)\n",
        "    pred_prob = model.predict(X_test_reshaped)\n",
        "    return np.argmax(pred_prob, axis=1)\n",
        "\n",
        "\n",
        "def predict_with_cnn(X_train, y_train, X_test, num_classes):\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "    X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "    model.fit(X_train_reshaped, y_train, epochs=50, batch_size=32, verbose=0)\n",
        "    pred_prob = model.predict(X_test_reshaped)\n",
        "    return np.argmax(pred_prob, axis=1)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "VW13kwNJ-e7L"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = {\n",
        "    \"Logistic Regression\": predict_with_logistic_regression(X_synthetic, y_synthetic_encoded, X_test),\n",
        "    \"CART\": predict_with_cart(X_synthetic, y_synthetic_encoded, X_test),\n",
        "    \"KNN\": predict_with_knn(X_synthetic, y_synthetic_encoded, X_test),\n",
        "    \"XGBoost\": predict_with_xgboost(X_synthetic, y_synthetic_encoded, X_test),\n",
        "    \"LSTM\": predict_with_lstm(X_synthetic, y_synthetic_cat, X_test, num_classes),\n",
        "    \"CNN\": predict_with_cnn(X_synthetic, y_synthetic_cat, X_test, num_classes)\n",
        "}\n",
        "\n",
        "\n",
        "# 성능 지표 저장을 위한 리스트\n",
        "results = []\n",
        "for model_name, pred_y in predictions.items():\n",
        "    accuracy = accuracy_score(y_test_encoded, pred_y)\n",
        "    recall = recall_score(y_test_encoded, pred_y, average='macro')\n",
        "    f1 = f1_score(y_test_encoded, pred_y, average='macro')\n",
        "    specificity = specificity_score(y_test_encoded, pred_y, average='macro')\n",
        "    conf_matrix = confusion_matrix(y_test_encoded, pred_y)\n",
        "    results.append([accuracy, f1, recall, specificity, conf_matrix])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7qd9sB6_vFk",
        "outputId": "19654e39-7164-4d29-b72c-99e744efa9d3"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [11:42:48] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 163ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "# 증강 전 클래스별 샘플 수\n",
        "original_class_distribution = Counter(y_train)\n",
        "print(\"증강 전 클래스별 샘플 수:\")\n",
        "for cls, count in original_class_distribution.items():\n",
        "    print(f\"클래스 {cls}: {count}개\")\n",
        "\n",
        "# 증강 후 클래스별 샘플 수\n",
        "augmented_class_distribution = Counter(y_synthetic)\n",
        "print(\"\\n증강 후 클래스별 샘플 수:\")\n",
        "for cls, count in augmented_class_distribution.items():\n",
        "    print(f\"클래스 {cls}: {count}개\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jFeYD96qNE5j",
        "outputId": "1819dd7f-de99-4214-c870-c173bbfe15c7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "증강 전 클래스별 샘플 수:\n",
            "클래스 1: 76개\n",
            "클래스 2: 31개\n",
            "클래스 3: 25개\n",
            "클래스 4: 32개\n",
            "클래스 5: 17개\n",
            "\n",
            "증강 후 클래스별 샘플 수:\n",
            "클래스 1: 76개\n",
            "클래스 2: 71개\n",
            "클래스 3: 71개\n",
            "클래스 4: 71개\n",
            "클래스 5: 71개\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 성능 지표를 DataFrame으로 변환\n",
        "results_df = pd.DataFrame(results, columns=[\"Accuracy\", \"F1\", \"Recall\", \"Specificity\", \"Confusion Matrix\"], index=predictions.keys())\n",
        "model_results = results_df.T\n",
        "# 결과를 출력\n",
        "print(\"\\n모델 성능 비교 결과:\")\n",
        "print(model_results)\n",
        "\n",
        "# 결과를 CSV 파일로 저장\n",
        "model_results.to_csv(\"/content/drive/My Drive/PhalangesOutlinesCorrect/results/(다중)tsmote_models_result.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ua7CnK0VBYzk",
        "outputId": "fd34c70c-cdd7-4371-e98e-a5f52e777a0f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "모델 성능 비교 결과:\n",
            "                       Logistic Regression                     CART  \\\n",
            "Accuracy                          0.632867                 0.656177   \n",
            "F1                                0.710212                 0.758395   \n",
            "Recall                             0.73384                 0.880228   \n",
            "Specificity                       0.472892                 0.301205   \n",
            "Confusion Matrix  [[157, 175], [140, 386]]  [[100, 232], [63, 463]]   \n",
            "\n",
            "                                      KNN                 XGBoost  \\\n",
            "Accuracy                         0.699301                0.678322   \n",
            "F1                               0.778351                0.786708   \n",
            "Recall                           0.861217                0.967681   \n",
            "Specificity                      0.442771                 0.21988   \n",
            "Confusion Matrix  [[147, 185], [73, 453]]  [[73, 259], [17, 509]]   \n",
            "\n",
            "                                      LSTM                       CNN  \n",
            "Accuracy                           0.65035                  0.602564  \n",
            "F1                                0.707031                  0.656596  \n",
            "Recall                            0.688213                  0.619772  \n",
            "Specificity                       0.590361                  0.575301  \n",
            "Confusion Matrix  [[196, 136], [164, 362]]  [[191, 141], [200, 326]]  \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}