{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1jW8Q59nFJlBJc6tnAOzFxjyL_rHvcIo9","authorship_tag":"ABX9TyNI17hEDHKico5LkCeOiMvu"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install tslearn"],"metadata":{"id":"8lpxL9MucNkP","executionInfo":{"status":"ok","timestamp":1746087779141,"user_tz":-540,"elapsed":9695,"user":{"displayName":"chulhyun hwang","userId":"06347918027293163200"}},"outputId":"6f853240-b9e6-4990-b8ea-1ef0ae008085","colab":{"base_uri":"https://localhost:8080/"}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting tslearn\n","  Downloading tslearn-0.6.3-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from tslearn) (2.0.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from tslearn) (1.15.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from tslearn) (1.6.1)\n","Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from tslearn) (0.60.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from tslearn) (1.4.2)\n","Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->tslearn) (0.43.0)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->tslearn) (3.6.0)\n","Downloading tslearn-0.6.3-py3-none-any.whl (374 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tslearn\n","Successfully installed tslearn-0.6.3\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0eEE3WXsbJAr"},"outputs":[],"source":["# -*- coding: utf-8 -*-\n","\n","# 필요한 라이브러리 임포트\n","import pandas as pd\n","import numpy as np\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from tslearn.metrics import dtw, cdist_dtw # tslearn: 시계열 DTW 거리 계산\n","from tslearn.neighbors import KNeighborsTimeSeriesClassifier # tslearn: DTW 기반 KNN\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import LSTM, Dense, Conv1D, MaxPooling1D, Flatten\n","from tensorflow.keras.utils import to_categorical\n","import random\n","import os\n","import xgboost as xgb\n","from collections import Counter\n","import tensorflow as tf\n","\n","# 시드 고정\n","seed_value = 42\n","os.environ['PYTHONHASHSEED'] = str(seed_value)\n","random.seed(seed_value)\n","np.random.seed(seed_value)\n","tf.random.set_seed(seed_value)\n","\n","# imblearn: 불균형 데이터 처리 관련 라이브러리\n","from imblearn.metrics import specificity_score # imblearn: 특이도(다수 클래스 재현율) 계산\n","\n","# 두 시계열 데이터 사이의 합성 샘플 생성 함수 (선형 보간)\n","def generate_synthetic_sample(ts1, ts2, alpha=0.5):\n","    return ts1 + alpha * (ts2 - ts1)\n","\n","# --- 데이터 로드 ---\n","print(\"데이터 로드 중...\")\n","# 제공해주신 경로에서 CSV 파일을 읽어옵니다.\n","df_X_train = pd.read_csv('/content/drive/My Drive/PhalangesOutlinesCorrect/X_train_sample2.csv')\n","df_y_train = pd.read_csv('/content/drive/My Drive/PhalangesOutlinesCorrect/y_train_sample2.csv')\n","df_X_test = pd.read_csv('/content/drive/MyDrive/PhalangesOutlinesCorrect/X_test_sample2.csv')\n","df_y_test = pd.read_csv('/content/drive/MyDrive/PhalangesOutlinesCorrect/y_test_sample2.csv')\n","print(\"데이터 로드 완료.\")\n","\n","# 데이터 numpy 배열로 변환 및 시계열 모델용 차원 추가\n","X_train = df_X_train.values\n","y_train = df_y_train.values.ravel()\n","X_test = df_X_test.values\n","y_test = df_y_test.values.ravel()\n","\n","# 시계열 모델 (LSTM, CNN, tslearn KNN)은 (samples, timesteps, features) 형태를 사용\n","X_train_ts = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n","X_test_ts = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n","\n","# 클래스 비율 확인 및 소수/다수 클래스 식별 (y_train 기준)\n","class_counts = Counter(y_train)\n","print(f\"\\n훈련 데이터 클래스 분포: {class_counts}\")\n","\n","sorted_classes = class_counts.most_common()\n","majority_class = sorted_classes[0][0]\n","minority_class = sorted_classes[-1][0]\n","\n","print(f\"소수 클래스: {minority_class}, 다수 클래스: {majority_class}\")\n","\n","# 소수 클래스 및 다수 클래스 인덱스 분리\n","minority_indices = np.where(y_train == minority_class)[0]\n","majority_indices = np.where(y_train == majority_class)[0]\n","\n","X_minority_ts = X_train_ts[minority_indices] # 소수 클래스 샘플 피처\n","X_majority_ts = X_train_ts[majority_indices] # 다수 클래스 샘플 피처\n","\n","# --- DTW 기반 Borderline SMOTE (안전 샘플 증강) ---\n","# k 값 설정 (이웃 탐색 개수)\n","k_neighbors = min(len(X_train_ts) - 1, 5)\n","\n","# 소수 클래스 샘플 vs 전체 훈련 데이터 DTW 거리 계산\n","dtw_dist_minority_all = cdist_dtw(X_minority_ts, X_train_ts, n_jobs=-1)\n","\n","# 소수 클래스 샘플을 '안전 샘플'로 분리 (k 이웃 중 소수 < 다수)\n","safe_minority_original_indices = []\n","\n","for i in range(len(X_minority_ts)):\n","    current_minority_dist_row = dtw_dist_minority_all[i, :]\n","    sorted_indices = np.argsort(current_minority_dist_row)\n","    k_neighbor_original_indices = sorted_indices[1 : k_neighbors + 1]\n","\n","    if len(k_neighbor_original_indices) > 0:\n","        k_neighbor_classes = y_train[k_neighbor_original_indices]\n","        num_minority_neighbors = np.sum(k_neighbor_classes == minority_class)\n","        num_majority_neighbors = np.sum(k_neighbor_classes == majority_class)\n","\n","        if num_minority_neighbors < num_majority_neighbors:\n","            safe_minority_original_indices.append(minority_indices[i])\n","\n","X_safe_minority_ts = X_train_ts[safe_minority_original_indices]\n","print(f\"안전 소수 클래스 샘플 수: {len(X_safe_minority_ts)}\")\n","\n","# 안전 샘플과 가장 가까운 다수 클래스 샘플 사이에 데이터 증강\n","synthetic_samples = []\n","synthetic_labels = []\n","\n","# 증강 데이터 변수를 원본 훈련 데이터로 초기화 (합성 데이터 없을 경우 대비)\n","X_augmented_ts = X_train_ts\n","y_augmented = y_train\n","\n","if len(X_safe_minority_ts) > 0:\n","    # 안전 소수 클래스 샘플 vs 다수 클래스 샘플 DTW 거리 계산\n","    dtw_dist_safe_minority_majority = cdist_dtw(X_safe_minority_ts, X_majority_ts, n_jobs=-1)\n","\n","    # 생성할 합성 데이터 총 개수 결정 (소수 클래스 수를 다수 클래스 수와 같게 만듦)\n","    target_minority_count = len(X_majority_ts)\n","    current_minority_count = len(X_minority_ts)\n","    num_samples_to_generate = target_minority_count - current_minority_count\n","\n","    if num_samples_to_generate > 0:\n","         samples_per_safe_sample_base = num_samples_to_generate // len(X_safe_minority_ts)\n","         remainder = num_samples_to_generate % len(X_safe_minority_ts)\n","\n","         for i in range(len(X_safe_minority_ts)):\n","            safe_sample_ts = X_safe_minority_ts[i]\n","            # 현재 안전 샘플에 가장 가까운 다수 클래스 샘플 찾기\n","            closest_majority_idx_in_majority_list = np.argmin(dtw_dist_safe_minority_majority[i, :])\n","            closest_majority_sample_ts = X_majority_ts[closest_majority_idx_in_majority_list]\n","\n","            # 현재 안전 샘플 당 생성할 샘플 개수 결정\n","            num_to_generate_this_sample = samples_per_safe_sample_base\n","            if remainder > 0:\n","                num_to_generate_this_sample += 1\n","                remainder -= 1\n","\n","            # 합성 샘플 생성 (선형 보간)\n","            for _ in range(num_to_generate_this_sample):\n","                alpha = random.random()\n","                synthetic_sample = generate_synthetic_sample(safe_sample_ts, closest_majority_sample_ts, alpha=alpha)\n","                synthetic_samples.append(synthetic_sample)\n","                synthetic_labels.append(minority_class)\n","\n","# 생성된 합성 데이터가 있으면 원래 데이터와 합치기\n","if synthetic_samples:\n","    X_synthetic_ts = np.array(synthetic_samples)\n","    y_synthetic = np.array(synthetic_labels)\n","    print(f\"생성된 합성 데이터 수: {len(X_synthetic_ts)}\")\n","    X_augmented_ts = np.concatenate([X_train_ts, X_synthetic_ts], axis=0)\n","    y_augmented = np.concatenate([y_train, y_synthetic], axis=0)\n","    print(f\"증강된 훈련 데이터 shape (시계열): {X_augmented_ts.shape}\")\n","    print(f\"증강된 훈련 레이블 shape: {y_augmented.shape}\")\n","\n","# 표준 모델 학습을 위한 데이터 (flattened) 준비\n","X_augmented = X_augmented_ts.reshape(X_augmented_ts.shape[0], -1)\n","X_test_flat = X_test_ts.reshape(X_test_ts.shape[0], -1)\n","\n","# --- 모델 정의 및 학습 함수 ---\n","\n","def predict_with_logistic_regression(X_train_data, y_train_data, X_test_data):\n","    print(\"Logistic Regression 모델 학습 중...\")\n","    model = LogisticRegression(max_iter=2000, solver='liblinear', random_state=42)\n","    model.fit(X_train_data, y_train_data)\n","    print(\"Logistic Regression 모델 학습 완료.\")\n","    return model.predict(X_test_data)\n","\n","def predict_with_cart(X_train_data, y_train_data, X_test_data):\n","    print(\"CART 모델 학습 중...\")\n","    model = DecisionTreeClassifier(random_state=42)\n","    model.fit(X_train_data, y_train_data)\n","    print(\"CART 모델 학습 완료.\")\n","    return model.predict(X_test_data)\n","\n","def predict_with_knn(X_train_data_ts, y_train_data, X_test_data_ts):\n","    print(\"KNN (DTW) 모델 학습 중...\")\n","    n_knn_neighbors = min(len(y_train_data), 5) if len(y_train_data) > 0 else 1\n","    if n_knn_neighbors < 1: n_knn_neighbors = 1\n","    model = KNeighborsTimeSeriesClassifier(n_neighbors=n_knn_neighbors, metric=\"dtw\", n_jobs=-1)\n","    model.fit(X_train_data_ts, y_train_data)\n","    print(\"KNN (DTW) 모델 학습 완료.\")\n","    return model.predict(X_test_data_ts)\n","\n","\n","def predict_with_xgboost(X_train_data, y_train_data, X_test_data):\n","    print(\"XGBoost 모델 학습 중...\")\n","    model = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', objective='binary:logistic', random_state=42)\n","    model.fit(X_train_data, y_train_data)\n","    print(\"XGBoost 모델 학습 완료.\")\n","    return model.predict(X_test_data)\n","\n","\n","def predict_with_lstm(X_train_data_ts, y_train_data, X_test_data_ts):\n","    print(\"LSTM 모델 학습 중...\")\n","    unique_train_classes = np.unique(y_train_data)\n","    all_possible_classes = np.unique(np.concatenate([y_train_data, y_test]))\n","    num_classes = len(all_possible_classes)\n","    y_train_one_hot = to_categorical(y_train_data, num_classes=num_classes)\n","\n","    input_shape = (X_train_data_ts.shape[1], X_train_data_ts.shape[2])\n","\n","    model = Sequential()\n","    model.add(LSTM(64, input_shape=input_shape, return_sequences=False))\n","    model.add(Dense(32, activation='relu'))\n","    model.add(Dense(num_classes, activation='softmax'))\n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","    model.fit(X_train_data_ts, y_train_one_hot, epochs=50, batch_size=32, verbose=0)\n","    print(\"LSTM 모델 학습 완료.\")\n","    pred_probs = model.predict(X_test_data_ts)\n","    return np.argmax(pred_probs, axis=1)\n","\n","\n","def predict_with_cnn(X_train_data_ts, y_train_data, X_test_data_ts):\n","    print(\"CNN 모델 학습 중...\")\n","    unique_train_classes = np.unique(y_train_data)\n","    all_possible_classes = np.unique(np.concatenate([y_train_data, y_test]))\n","    num_classes = len(all_possible_classes)\n","    y_train_one_hot = to_categorical(y_train_data, num_classes=num_classes)\n","\n","    input_shape = (X_train_data_ts.shape[1], X_train_data_ts.shape[2])\n","\n","    model = Sequential()\n","    model.add(Conv1D(filters=64, kernel_size=5, activation='relu', input_shape=input_shape))\n","    model.add(MaxPooling1D(pool_size=2))\n","    model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n","    model.add(MaxPooling1D(pool_size=2))\n","    model.add(Flatten())\n","    model.add(Dense(64, activation='relu'))\n","    model.add(Dense(num_classes, activation='softmax'))\n","    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","    model.fit(X_train_data_ts, y_train_one_hot, epochs=50, batch_size=32, verbose=0)\n","    print(\"CNN 모델 학습 완료.\")\n","    pred_probs = model.predict(X_test_data_ts)\n","    return np.argmax(pred_probs, axis=1)\n","\n","\n","# --- 예측 결과 저장 및 성능 평가 ---\n","\n","print(\"\\n\\n--- 모델별 성능 평가 결과 ---\")\n","\n","predictions = {}\n","\n","X_train_for_flat = X_augmented\n","y_train_for_flat = y_augmented\n","X_test_for_flat = X_test_flat\n","X_train_for_ts = X_augmented_ts\n","y_train_for_ts = y_augmented\n","X_test_for_ts = X_test_ts\n","\n","predictions[\"Logistic Regression\"] = predict_with_logistic_regression(X_train_for_flat, y_train_for_flat, X_test_for_flat)\n","predictions[\"CART\"] = predict_with_cart(X_train_for_flat, y_train_for_flat, X_test_for_flat)\n","predictions[\"KNN (DTW)\"] = predict_with_knn(X_train_for_ts, y_train_for_ts, X_test_for_ts)\n","predictions[\"XGBoost\"] = predict_with_xgboost(X_train_for_flat, y_train_for_flat, X_test_for_flat)\n","predictions[\"LSTM\"] = predict_with_lstm(X_train_for_ts, y_train_for_ts, X_test_for_ts)\n","predictions[\"CNN\"] = predict_with_cnn(X_train_for_ts, y_train_for_ts, X_test_for_ts)\n","\n"]},{"cell_type":"code","source":["from sklearn.model_selection import StratifiedKFold\n","from sklearn.ensemble import RandomForestClassifier"],"metadata":{"id":"tNOKuTxN_gmB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 11. 교차 검증 및 모델 학습\n","best_threshold = None\n","best_specificity = 0\n","best_model = None\n","\n","for fold, (train_index, val_index) in enumerate(skf.split(X_augmented, y_augmented)): #X_train_resampled, y_train_resampled로 학습\n","    print(f\"Fold {fold+1}\")\n","\n","    # 데이터 분할\n","    X_train_fold, X_val_fold = X_augmented[train_index], X_augmented[val_index] #X_train_resampled, y_train_resampled로 학습\n","    y_train_fold, y_val_fold = y_augmented[train_index], y_augmented[val_index] #X_train_resampled, y_train_resampled로 학습\n","\n","    rf_model = RandomForestClassifier(random_state=42)\n","    # 모델 학습\n","    rf_model.fit(X_train_fold, y_train_fold)\n","\n","    # 검증 데이터 예측 (확률값 예측)\n","    y_pred_proba = rf_model.predict_proba(X_val_fold)[:, 1]\n","\n","    # 최적 임계값 찾기 (Specificity 최대화)\n","    thresholds = np.linspace(0, 1, 100)  # 임계값 후보 생성\n","    for threshold in thresholds:\n","        y_pred_temp = (y_pred_proba >= threshold).astype(int)\n","        specificity = specificity_score(y_val_fold, y_pred_temp)\n","        if specificity > best_specificity:\n","            best_specificity = specificity\n","            best_threshold = threshold\n","            best_model = rf_model # 현재 fold의 모델 저장\n","\n","print(f\"Best Threshold: {best_threshold}\")\n","\n","# 12. 테스트 데이터 예측 (최적 모델 및 임계값 사용)\n","y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n","y_pred = (y_pred_proba >= best_threshold).astype(int)"],"metadata":{"id":"gZQ5DYkD67fP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for model_name, pred_y in predictions.items():\n","    accuracy = accuracy_score(y_test, pred_y)\n","    # Recall: 소수 클래스(minority_class)에 대한 재현율\n","    recall = recall_score(y_test, pred_y, pos_label=minority_class, zero_division=0)\n","    # F1-Score: 소수 클래스(minority_class)에 대한 F1 점수\n","    f1 = f1_score(y_test, pred_y, pos_label=minority_class, zero_division=0)\n","\n","    # Specificity: 다수 클래스(majority_class)에 대한 재현율 (True Negative Rate)\n","    specificity = specificity_score(y_test, pred_y, pos_label=majority_class)\n","\n","    conf_matrix = confusion_matrix(y_test, pred_y)\n","\n","    print(f\"\\n--- {model_name} 모델 성능 ---\")\n","    print(\"모델 성능 평가:\")\n","    print(f\"  정확도: {accuracy:.4f}\")\n","    print(f\"  F1-Score (소수 클래스 {minority_class}): {f1:.4f}\")\n","    print(f\"  재현율 (Recall, 소수 클래스 {minority_class}): {recall:.4f}\")\n","    print(f\"  특이도 (Specificity, 다수 클래스 {majority_class}): {specificity:.4f}\")\n","    print(\"  혼동 행렬 (Confusion Matrix):\")\n","    print(conf_matrix)\n","\n","    # Confusion Matrix 시각화\n","    plt.figure(figsize=(4, 3))\n","    cm = confusion_matrix(y_test, pred_y)\n","    all_labels_unique = np.unique(np.concatenate([y_test, pred_y]))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","                xticklabels=all_labels_unique, yticklabels=all_labels_unique)\n","    plt.xlabel('Predicted Label')\n","    plt.ylabel('True Label')\n","    plt.title(f'{model_name} Confusion Matrix')\n","    plt.show()"],"metadata":{"id":"GcaFs5Lmg8NK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["y_pred_proba_final = best_model.predict_proba(X_test)[:, 1]\n","y_pred = (y_pred_proba_final >= best_threshold).astype(int) # y_pred 변수에 최종 예측 결과 저장\n","\n","print(\"테스트 데이터 예측 완료.\")\n","\n","# --- 예측 결과에 대한 성능 평가 및 시각화 ---\n","# 성능 평가 및 시각화를 위해 사용할 모델 이름과 예측 결과를 정의합니다.\n","model_name = 'Cross-validated Random Forest' # 모델 이름\n","pred_y = y_pred # 예측 결과\n","\n","print(f\"\\n--- {model_name} 모델 성능 ---\")\n","print(\"모델 성능 평가:\")\n","\n","# 성능 지표 계산 (zero_division=0 추가)\n","accuracy = accuracy_score(y_test, pred_y)\n","recall = recall_score(y_test, pred_y, pos_label=minority_class, zero_division=0)\n","f1 = f1_score(y_test, pred_y, pos_label=minority_class, zero_division=0)\n","specificity = specificity_score(y_test, pred_y, pos_label=majority_class)\n","conf_matrix = confusion_matrix(y_test, pred_y)\n","\n","# 성능 지표 출력\n","print(f\"  정확도: {accuracy:.4f}\")\n","print(f\"  F1-Score (소수 클래스 {minority_class}): {f1:.4f}\")\n","print(f\"  재현율 (Recall, 소수 클래스 {minority_class}): {recall:.4f}\")\n","print(f\"  특이도 (Specificity, 다수 클래스 {majority_class}): {specificity:.4f}\")\n","print(\"  혼동 행렬 (Confusion Matrix):\")\n","print(conf_matrix)\n","\n","# 혼동 행렬 시각화\n","plt.figure(figsize=(4, 3))\n","cm = confusion_matrix(y_test, pred_y)\n","# 혼동 행렬 레이블을 실제 데이터의 고유 레이블로 설정\n","unique_labels = np.unique(np.concatenate([y_test, pred_y]))\n","sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","            xticklabels=unique_labels, yticklabels=unique_labels)\n","plt.xlabel('Predicted Label')\n","plt.ylabel('True Label')\n","plt.title(f'{model_name} Confusion Matrix')\n","plt.show()\n","\n","print(\"\\n모델 성능 평가 및 시각화 완료.\")"],"metadata":{"id":"j_WzloaNCe6z"},"execution_count":null,"outputs":[]}]}
