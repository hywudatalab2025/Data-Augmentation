{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmUy6FX0VzyY",
        "outputId": "4740a272-4159-4bb8-9b64-37b97ca89736"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Google Drive 마운트\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "g_4o0x2LWVIr"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. 데이터 로드\n",
        "df_X_train = pd.read_csv('/content/drive/My Drive/PhalangesOutlinesCorrect/X_train_sample2.csv')\n",
        "df_y_train = pd.read_csv('/content/drive/My Drive/PhalangesOutlinesCorrect/y_train_sample2.csv')\n",
        "df_X_test = pd.read_csv('/content/drive/MyDrive/PhalangesOutlinesCorrect/X_test_original.csv')\n",
        "df_y_test = pd.read_csv('/content/drive/MyDrive/PhalangesOutlinesCorrect/y_test_original.csv')"
      ],
      "metadata": {
        "id": "mbVldInQWT2D"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SfgIA86uaHb2",
        "outputId": "2b73e351-4c84-4649-ea16-b9679dda6f4a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "246"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train = df_X_train.values\n",
        "y_train = df_y_train.values.reshape(-1)  # 1차원 배열(벡터)로 변환\n",
        "X_test = df_X_test.values\n",
        "y_test = df_y_test.values.reshape(-1)  # 1차원 배열(벡터)로 변환"
      ],
      "metadata": {
        "id": "anx0MjCgXMhi"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 소수 클래스와 다수 클래스 식별\n",
        "target_counts = pd.Series(y_train).value_counts()\n",
        "minority_class = target_counts.idxmin()\n",
        "majority_class = target_counts.idxmax()\n",
        "\n",
        "# 소수 클래스 샘플 추출\n",
        "X_minority = X_train[y_train == minority_class]\n",
        "X_major = X_train[y_train == majority_class]"
      ],
      "metadata": {
        "id": "-uNRFn7lXNd_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dtaidistance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFyIRUFHa6tb",
        "outputId": "d74fba57-0e25-40d8-d40f-d443702d2832"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: dtaidistance in /usr/local/lib/python3.11/dist-packages (2.3.13)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from dtaidistance) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dtaidistance import dtw\n",
        "from scipy.spatial.distance import euclidean\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from dtaidistance.dtw import distance  # 함수 이름은 distance임"
      ],
      "metadata": {
        "id": "cCKDL7GDY24o"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dtaidistance.dtw import distance as dtw_distance  # 함수 이름을 바꿔서 import\n",
        "\n",
        "def calculate_all_dtw_distances(X):\n",
        "    distances = []\n",
        "    n = len(X)\n",
        "    for i in range(n):\n",
        "        for j in range(i + 1, n):\n",
        "            dist_val = dtw_distance(X[i], X[j])  # 함수 호출\n",
        "            distances.append((i, j, dist_val))   # 결과 저장\n",
        "    return distances\n"
      ],
      "metadata": {
        "id": "KNwyedhQZAXQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DTW 거리 계산 실행\n",
        "X_all = np.vstack((X_minority, X_major))  # 전체 학습 데이터\n",
        "y_all = np.hstack((np.full(len(X_minority), minority_class), np.full(len(X_major), majority_class)))\n",
        "\n",
        "print(f\"전체 샘플 수: {len(X_all)}, 소수 클래스: {minority_class}, 다수 클래스: {majority_class}\")\n",
        "\n",
        "# 모든 샘플 간의 DTW 거리 계산\n",
        "dtw_distances = calculate_all_dtw_distances(X_all)\n",
        "\n",
        "# 거리 행렬 초기화\n",
        "n_samples = len(X_all)\n",
        "dtw_matrix = np.zeros((n_samples, n_samples))\n",
        "\n",
        "# 거리 행렬 채우기\n",
        "for i, j, dist_val in dtw_distances:\n",
        "    dtw_matrix[i, j] = dist_val\n",
        "    dtw_matrix[j, i] = dist_val  # 대칭 행렬\n",
        "\n",
        "print(\"DTW 거리 행렬 계산 완료:\", dtw_matrix.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWJoVxUicHuF",
        "outputId": "f220d32c-b0b6-4432-a9d2-ea85fc637c46"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 샘플 수: 246, 소수 클래스: 0, 다수 클래스: 1\n",
            "DTW 거리 행렬 계산 완료: (246, 246)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = 3  # 홀수로 설정 (예: 3, 5, 7 등)\n",
        "\n",
        "safe_minority_indices = []  # 안전 샘플 인덱스\n",
        "risky_minority_indices = []  # 위험 샘플 인덱스\n",
        "\n",
        "minority_indices = np.where(y_all == minority_class)[0]\n",
        "majority_indices = np.where(y_all == majority_class)[0]\n",
        "\n",
        "for idx in minority_indices:\n",
        "    distances = dtw_matrix[idx]\n",
        "\n",
        "    # 자기 자신 제외하고 가까운 이웃 k개 선택\n",
        "    nearest_indices = np.argsort(distances)[1:k+1]\n",
        "\n",
        "    # 이웃의 클래스 확인\n",
        "    neighbor_labels = y_all[nearest_indices]\n",
        "    minority_count = np.sum(neighbor_labels == minority_class)\n",
        "    majority_count = np.sum(neighbor_labels == majority_class)\n",
        "\n",
        "    # 안전 / 위험 분류\n",
        "    if minority_count < majority_count:\n",
        "        safe_minority_indices.append(idx)\n",
        "    else:\n",
        "        risky_minority_indices.append(idx)\n",
        "\n",
        "print(f\"총 소수 클래스 수: {len(minority_indices)}\")\n",
        "print(f\"안전 샘플 수: {len(safe_minority_indices)}\")\n",
        "print(f\"위험 샘플 수: {len(risky_minority_indices)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7ms5v6zcSEl",
        "outputId": "16394a20-431b-48ca-8012-f7cd0f262c5a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 소수 클래스 수: 12\n",
            "안전 샘플 수: 11\n",
            "위험 샘플 수: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 증강할 데이터 수\n",
        "n_augment = len(X_major) - len(X_minority)  # 다수 - 소수\n",
        "n_safe = len(safe_minority_indices)\n",
        "\n",
        "# 한 쌍에서 생성해야 할 수\n",
        "n_per_pair = n_augment // n_safe\n",
        "remainder = n_augment % n_safe\n",
        "\n",
        "print(f\"총 생성할 샘플 수: {n_augment} (각 쌍당 {n_per_pair}개, 남는 수 {remainder})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UEj-wjk7l53Q",
        "outputId": "87d71f76-fe0f-42d5-95cc-de892a3e39b9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 생성할 샘플 수: 222 (각 쌍당 20개, 남는 수 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "augmented_data = []\n",
        "random.seed(42)\n",
        "\n",
        "# 안전 샘플 쌍 조합을 만들고 증강\n",
        "for i, idx in enumerate(safe_minority_indices):\n",
        "    for _ in range(n_per_pair):\n",
        "        # 다른 안전 샘플 하나 무작위 선택\n",
        "        other_idx = random.choice([j for j in safe_minority_indices if j != idx])\n",
        "\n",
        "        # 선형 보간 (alpha: 0~1 사이 랜덤)\n",
        "        alpha = np.random.rand()\n",
        "        new_sample = alpha * X_all[idx] + (1 - alpha) * X_all[other_idx]\n",
        "        augmented_data.append(new_sample)\n",
        "\n",
        "# 남는 remainder 개수도 랜덤하게 더 생성\n",
        "for _ in range(remainder):\n",
        "    idx1, idx2 = random.sample(safe_minority_indices, 2)\n",
        "    alpha = np.random.rand()\n",
        "    new_sample = alpha * X_all[idx1] + (1 - alpha) * X_all[idx2]\n",
        "    augmented_data.append(new_sample)\n",
        "\n",
        "# 최종 증강 데이터와 레이블 결합\n",
        "X_aug = np.array(augmented_data)\n",
        "y_aug = np.full(len(X_aug), minority_class)\n",
        "\n",
        "# 기존 학습 데이터와 결합\n",
        "X_synthetic = np.vstack((X_all, X_aug))\n",
        "y_synthetic = np.hstack((y_all, y_aug))\n",
        "\n",
        "print(\"증강 완료!\")\n",
        "print(f\"최종 데이터 크기: {X_synthetic.shape}, 레이블 분포: {pd.Series(y_synthetic).value_counts().to_dict()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ZwUFweEl42M",
        "outputId": "06f70734-1321-41f3-df77-45aef6224820"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "증강 완료!\n",
            "최종 데이터 크기: (468, 80), 레이블 분포: {0: 234, 1: 234}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 교차 검증을 위한 설정\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix, roc_auc_score\n",
        "from sklearn.metrics import precision_score, make_scorer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Conv1D, Flatten\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from imblearn.metrics import specificity_score # specificity_score 임포트\n"
      ],
      "metadata": {
        "id": "b5ZsP01ZmzDE"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_with_logistic_regression(X_train, y_train, X_test):\n",
        "    model = LogisticRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "    return model.predict(X_test)\n",
        "\n",
        "def predict_with_cart(X_train, y_train, X_test):\n",
        "    model = DecisionTreeClassifier()\n",
        "    model.fit(X_train, y_train)\n",
        "    return model.predict(X_test)\n",
        "\n",
        "def predict_with_knn(X_train, y_train, X_test, k=3):\n",
        "    model = KNeighborsClassifier(n_neighbors=k)\n",
        "    model.fit(X_train, y_train)\n",
        "    return model.predict(X_test)\n",
        "\n",
        "def predict_with_xgboost(X_train, y_train, X_test):\n",
        "    model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "    model.fit(X_train, y_train)\n",
        "    return model.predict(X_test)\n",
        "\n",
        "def predict_with_lstm(X_train, y_train, X_test):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(50, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "    X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "    model.fit(X_train_reshaped, y_train, epochs=50, batch_size=32, verbose=0)\n",
        "    return (model.predict(X_test_reshaped) > 0.5).astype(int)\n",
        "\n",
        "def predict_with_cnn(X_train, y_train, X_test):\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "    X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "    model.fit(X_train_reshaped, y_train, epochs=50, batch_size=32, verbose=0)\n",
        "    return (model.predict(X_test_reshaped) > 0.5).astype(int)\n"
      ],
      "metadata": {
        "id": "Jrg_DxJznAR4"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측 실행 및 평가\n",
        "predictions = {\n",
        "    \"Logistic Regression\": predict_with_logistic_regression(X_synthetic, y_synthetic, X_test),\n",
        "    \"CART\": predict_with_cart(X_synthetic, y_synthetic, X_test),\n",
        "    \"KNN\": predict_with_knn(X_synthetic, y_synthetic, X_test),\n",
        "    \"XGBoost\": predict_with_xgboost(X_synthetic, y_synthetic, X_test),\n",
        "    \"LSTM\": predict_with_lstm(X_synthetic, y_synthetic, X_test),\n",
        "    \"CNN\": predict_with_cnn(X_synthetic, y_synthetic, X_test)\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_lPfW66W4O7v",
        "outputId": "df48dce1-6a38-487e-d682-97669d430802"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:37:54] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 성능 지표 저장을 위한 리스트\n",
        "results = []\n",
        "\n",
        "for model_name, pred_y in predictions.items():\n",
        "    accuracy = accuracy_score(y_test, pred_y)\n",
        "    recall = recall_score(y_test, pred_y)\n",
        "    f1 = f1_score(y_test, pred_y)\n",
        "    specificity = specificity_score(y_test, pred_y)\n",
        "    conf_matrix = confusion_matrix(y_test, pred_y)\n",
        "    results.append([accuracy, f1, recall, specificity, conf_matrix])\n"
      ],
      "metadata": {
        "id": "S-gtMPNYmI2h"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 성능 지표를 DataFrame으로 변환\n",
        "results_df = pd.DataFrame(results, columns=[\"Accuracy\", \"F1\", \"Recall\", \"Specificity\", \"Confusion Matrix\"], index=predictions.keys())\n",
        "model_results = results_df.T\n",
        "# 결과를 출력\n",
        "print(\"\\n모델 성능 비교 결과:\")\n",
        "print(model_results)\n",
        "\n",
        "# 결과를 CSV 파일로 저장\n",
        "model_results.to_csv(\"/content/drive/My Drive/PhalangesOutlinesCorrect/results/bsmote_soso_result.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qLCIplqIC2qu",
        "outputId": "530b7404-e201-4c6a-a559-d6f99f672c4b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "모델 성능 비교 결과:\n",
            "                     Logistic Regression                    CART  \\\n",
            "Accuracy                        0.611888                 0.62704   \n",
            "F1                              0.740856                0.752705   \n",
            "Recall                          0.904943                0.925856   \n",
            "Specificity                      0.14759                0.153614   \n",
            "Confusion Matrix  [[49, 283], [50, 476]]  [[51, 281], [39, 487]]   \n",
            "\n",
            "                                      KNN                 XGBoost  \\\n",
            "Accuracy                         0.681818                0.648019   \n",
            "F1                               0.769231                0.773273   \n",
            "Recall                           0.865019                0.979087   \n",
            "Specificity                      0.391566                0.123494   \n",
            "Confusion Matrix  [[130, 202], [71, 455]]  [[41, 291], [11, 515]]   \n",
            "\n",
            "                                      LSTM                     CNN  \n",
            "Accuracy                          0.547786                 0.63986  \n",
            "F1                                0.598344                0.757647  \n",
            "Recall                             0.54943                0.918251  \n",
            "Specificity                       0.545181                0.198795  \n",
            "Confusion Matrix  [[181, 151], [237, 289]]  [[66, 266], [43, 483]]  \n"
          ]
        }
      ]
    }
  ]
}