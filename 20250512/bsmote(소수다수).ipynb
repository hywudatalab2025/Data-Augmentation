{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "om4ittXxN-O3",
        "outputId": "9b203789-aebf-492d-e4ed-d43d257960b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dtaidistance"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z_XVB1lsvdJl",
        "outputId": "8ffb2701-ddeb-435d-929e-4e3d08c34ca0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dtaidistance\n",
            "  Downloading dtaidistance-2.3.13-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from dtaidistance) (2.0.2)\n",
            "Downloading dtaidistance-2.3.13-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dtaidistance\n",
            "Successfully installed dtaidistance-2.3.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "qiyg-51Yus5e"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "dfW7fkstmalu"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, recall_score, confusion_matrix, roc_curve, auc\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from imblearn.over_sampling import ADASYN\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import matplotlib.pyplot as plt\n",
        "from imblearn.metrics import specificity_score # specificity_score 임포트\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Conv1D, Flatten\n",
        "import seaborn as sns\n",
        "from scipy.spatial.distance import euclidean\n",
        "import random\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PBqLiRHQmfcM"
      },
      "outputs": [],
      "source": [
        "# 1. 데이터 로드\n",
        "df_X_train = pd.read_csv('/content/drive/My Drive/PhalangesOutlinesCorrect/X_train_sample2.csv')\n",
        "df_y_train = pd.read_csv('/content/drive/My Drive/PhalangesOutlinesCorrect/y_train_sample2.csv')\n",
        "df_X_test = pd.read_csv('/content/drive/MyDrive/PhalangesOutlinesCorrect/X_test_original.csv')\n",
        "df_y_test = pd.read_csv('/content/drive/MyDrive/PhalangesOutlinesCorrect/y_test_original.csv')\n",
        "\n",
        "\n",
        "X_train = df_X_train.values\n",
        "y_train = df_y_train.values.reshape(-1)  # 1차원 배열(벡터)로 변환\n",
        "X_test = df_X_test.values\n",
        "y_test = df_y_test.values.reshape(-1)  # 1차원 배열(벡터)로 변환\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 소수 클래스와 다수 클래스 식별\n",
        "target_counts = pd.Series(y_train).value_counts()\n",
        "minority_class = target_counts.idxmin()\n",
        "majority_class = target_counts.idxmax()\n",
        "\n",
        "# 소수 클래스 샘플 추출\n",
        "X_minority = X_train[y_train == minority_class]\n",
        "X_major = X_train[y_train == majority_class]"
      ],
      "metadata": {
        "id": "MmdUpOp6u_CZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dtaidistance import dtw\n",
        "from scipy.spatial.distance import euclidean\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from dtaidistance.dtw import distance  # 함수 이름은 distance임"
      ],
      "metadata": {
        "id": "cbizkKG8D4Ai"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dtaidistance.dtw import distance as dtw_distance  # 함수 이름을 바꿔서 import\n",
        "\n",
        "def calculate_all_dtw_distances(X):\n",
        "    distances = []\n",
        "    n = len(X)\n",
        "    for i in range(n):\n",
        "        for j in range(i + 1, n):\n",
        "            dist_val = dtw_distance(X[i], X[j])  # 함수 호출\n",
        "            distances.append((i, j, dist_val))   # 결과 저장\n",
        "    return distances\n"
      ],
      "metadata": {
        "id": "j-7dNg2bD5XT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DTW 거리 계산 실행\n",
        "X_all = np.vstack((X_minority, X_major))  # 전체 학습 데이터\n",
        "y_all = np.hstack((np.full(len(X_minority), minority_class), np.full(len(X_major), majority_class)))\n",
        "\n",
        "print(f\"전체 샘플 수: {len(X_all)}, 소수 클래스: {minority_class}, 다수 클래스: {majority_class}\")\n",
        "\n",
        "# 모든 샘플 간의 DTW 거리 계산\n",
        "dtw_distances = calculate_all_dtw_distances(X_all)\n",
        "\n",
        "# 거리 행렬 초기화\n",
        "n_samples = len(X_all)\n",
        "dtw_matrix = np.zeros((n_samples, n_samples))\n",
        "\n",
        "# 거리 행렬 채우기\n",
        "for i, j, dist_val in dtw_distances:\n",
        "    dtw_matrix[i, j] = dist_val\n",
        "    dtw_matrix[j, i] = dist_val  # 대칭 행렬\n",
        "\n",
        "print(\"DTW 거리 행렬 계산 완료:\", dtw_matrix.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_KREuEED8Fu",
        "outputId": "a015b729-0c4d-46e2-ddcc-5924a588df21"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "전체 샘플 수: 246, 소수 클래스: 0, 다수 클래스: 1\n",
            "DTW 거리 행렬 계산 완료: (246, 246)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "k = 3  # 홀수로 설정 (예: 3, 5, 7 등)\n",
        "\n",
        "safe_minority_indices = []  # 안전 샘플 인덱스\n",
        "risky_minority_indices = []  # 위험 샘플 인덱스\n",
        "\n",
        "minority_indices = np.where(y_all == minority_class)[0]\n",
        "majority_indices = np.where(y_all == majority_class)[0]\n",
        "\n",
        "for idx in minority_indices:\n",
        "    distances = dtw_matrix[idx]\n",
        "\n",
        "    # 자기 자신 제외하고 가까운 이웃 k개 선택\n",
        "    nearest_indices = np.argsort(distances)[1:k+1]\n",
        "\n",
        "    # 이웃의 클래스 확인\n",
        "    neighbor_labels = y_all[nearest_indices]\n",
        "    minority_count = np.sum(neighbor_labels == minority_class)\n",
        "    majority_count = np.sum(neighbor_labels == majority_class)\n",
        "\n",
        "    # 안전 / 위험 분류\n",
        "    if minority_count < majority_count:\n",
        "        safe_minority_indices.append(idx)\n",
        "    else:\n",
        "        risky_minority_indices.append(idx)\n",
        "\n",
        "print(f\"총 소수 클래스 수: {len(minority_indices)}\")\n",
        "print(f\"안전 샘플 수: {len(safe_minority_indices)}\")\n",
        "print(f\"위험 샘플 수: {len(risky_minority_indices)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGzVUdVQEOmH",
        "outputId": "e4d12081-6d2c-4699-9802-34d2b4fa6ce9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "총 소수 클래스 수: 12\n",
            "안전 샘플 수: 11\n",
            "위험 샘플 수: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. 안전 샘플에 해당하는 소수 클래스 샘플만을 가지고 증강 준비\n",
        "X_safe_minority = X_all[safe_minority_indices]\n",
        "\n",
        "# 7. 증강할 데이터 수 계산\n",
        "n_major = len(X_major)\n",
        "n_minority = len(X_minority)\n",
        "n_safe = len(X_safe_minority)\n",
        "n_augment = n_major - n_minority\n",
        "\n",
        "print(f\"증강해야 할 샘플 수: {n_augment}, 안전 소수 클래스 샘플 수: {n_safe}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWejcaETHyxq",
        "outputId": "7fec0d55-8379-4821-bdf8-c449597dc513"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "증강해야 할 샘플 수: 222, 안전 소수 클래스 샘플 수: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 8. 한 쌍에서 생성할 수와 나머지 계산\n",
        "n_per_pair = n_augment // n_safe\n",
        "remainder = n_augment % n_safe\n",
        "print(f\"각 안전 샘플당 생성 수: {n_per_pair}, 남는 개수: {remainder}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-c4imgqBH9R8",
        "outputId": "6f5bdcf7-db13-4e32-d464-6af5cf404106"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "각 안전 샘플당 생성 수: 20, 남는 개수: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_samples = []\n",
        "\n",
        "for idx, safe_idx in enumerate(safe_minority_indices):\n",
        "    safe_sample = X_all[safe_idx]\n",
        "\n",
        "    # DTW 거리 기반 이웃 찾기\n",
        "    distances = dtw_matrix[safe_idx]\n",
        "    nearest_indices = np.argsort(distances)[1:k+1]  # 자기 자신 제외, k개의 이웃\n",
        "\n",
        "    # 이웃 중 다수 클래스에 해당하는 인덱스만 추출\n",
        "    majority_neighbors = [i for i in nearest_indices if y_all[i] == majority_class]\n",
        "\n",
        "    if not majority_neighbors:\n",
        "        continue  # 다수 클래스 이웃이 없다면 증강 생략\n",
        "\n",
        "    for _ in range(n_per_pair + (1 if idx < remainder else 0)):\n",
        "        # 이웃 중 다수 클래스 샘플 무작위 선택\n",
        "        majority_neighbor_idx = random.choice(majority_neighbors)\n",
        "        majority_sample = X_all[majority_neighbor_idx]\n",
        "\n",
        "        # 랜덤 선형 보간\n",
        "        alpha = np.random.rand()\n",
        "        synthetic_sample = alpha * safe_sample + (1 - alpha) * majority_sample\n",
        "        augmented_samples.append(synthetic_sample)\n"
      ],
      "metadata": {
        "id": "aMk_TazTICHs"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 12. 최종 증강 데이터와 레이블 결합\n",
        "X_aug = np.array(augmented_samples)\n",
        "y_aug = np.full(len(X_aug), minority_class)\n",
        "\n",
        "print(f\"증강된 샘플 수: {len(X_aug)}\")\n",
        "\n",
        "# 13. 기존 학습 데이터와 결합\n",
        "X_train_augmented = np.vstack((X_train, X_aug))\n",
        "y_train_augmented = np.hstack((y_train, y_aug))\n",
        "\n",
        "print(\"증강된 학습 데이터 최종 크기:\", X_train_augmented.shape, y_train_augmented.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ekryQKAIIF2d",
        "outputId": "3198f55f-19a5-4572-cd50-c59f38babfca"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "증강된 샘플 수: 222\n",
            "증강된 학습 데이터 최종 크기: (468, 80) (468,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_with_logistic_regression(X_train, y_train, X_test):\n",
        "    model = LogisticRegression()\n",
        "    model.fit(X_train, y_train)\n",
        "    return model.predict(X_test)\n",
        "\n",
        "def predict_with_cart(X_train, y_train, X_test):\n",
        "    model = DecisionTreeClassifier()\n",
        "    model.fit(X_train, y_train)\n",
        "    return model.predict(X_test)\n",
        "\n",
        "def predict_with_knn(X_train, y_train, X_test, k=3):\n",
        "    model = KNeighborsClassifier(n_neighbors=k)\n",
        "    model.fit(X_train, y_train)\n",
        "    return model.predict(X_test)\n",
        "\n",
        "def predict_with_xgboost(X_train, y_train, X_test):\n",
        "    model = XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "    model.fit(X_train, y_train)\n",
        "    return model.predict(X_test)\n",
        "\n",
        "def predict_with_lstm(X_train, y_train, X_test):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(50, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "    X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "    model.fit(X_train_reshaped, y_train, epochs=50, batch_size=32, verbose=0)\n",
        "    return (model.predict(X_test_reshaped) > 0.5).astype(int)\n",
        "\n",
        "def predict_with_cnn(X_train, y_train, X_test):\n",
        "    model = Sequential()\n",
        "    model.add(Conv1D(64, kernel_size=3, activation='relu', input_shape=(X_train.shape[1], 1)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    X_train_reshaped = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
        "    X_test_reshaped = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "    model.fit(X_train_reshaped, y_train, epochs=50, batch_size=32, verbose=0)\n",
        "    return (model.predict(X_test_reshaped) > 0.5).astype(int)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "VW13kwNJ-e7L"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 예측 실행 및 평가\n",
        "predictions = {\n",
        "    \"Logistic Regression\": predict_with_logistic_regression(X_train_augmented, y_train_augmented, X_test),\n",
        "    \"CART\": predict_with_cart(X_train_augmented, y_train_augmented, X_test),\n",
        "    \"KNN\": predict_with_knn(X_train_augmented, y_train_augmented, X_test),\n",
        "    \"XGBoost\": predict_with_xgboost(X_train_augmented, y_train_augmented, X_test),\n",
        "    \"LSTM\": predict_with_lstm(X_train_augmented, y_train_augmented, X_test),\n",
        "    \"CNN\": predict_with_cnn(X_train_augmented, y_train_augmented, X_test)\n",
        "}\n",
        "# 성능 지표 저장을 위한 리스트\n",
        "results = []\n",
        "\n",
        "for model_name, pred_y in predictions.items():\n",
        "    accuracy = accuracy_score(y_test, pred_y)\n",
        "    recall = recall_score(y_test, pred_y)\n",
        "    f1 = f1_score(y_test, pred_y)\n",
        "    specificity = specificity_score(y_test, pred_y)\n",
        "    conf_matrix = confusion_matrix(y_test, pred_y)\n",
        "    results.append([accuracy, f1, recall, specificity, conf_matrix])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w7qd9sB6_vFk",
        "outputId": "2998e0a3-0409-458c-83f8-08394a52e535"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [10:27:33] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m27/27\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 성능 지표를 DataFrame으로 변환\n",
        "results_df = pd.DataFrame(results, columns=[\"Accuracy\", \"F1\", \"Recall\", \"Specificity\", \"Confusion Matrix\"], index=predictions.keys())\n",
        "model_results = results_df.T\n",
        "# 결과를 출력\n",
        "print(\"\\n모델 성능 비교 결과:\")\n",
        "print(model_results)\n",
        "\n",
        "# 결과를 CSV 파일로 저장\n",
        "model_results.to_csv(\"/content/drive/My Drive/PhalangesOutlinesCorrect/results/bsmote_soda_result.csv\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ua7CnK0VBYzk",
        "outputId": "7e402a9d-6bb9-4246-faf8-baa8393ca0a7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "모델 성능 비교 결과:\n",
            "                     Logistic Regression                     CART  \\\n",
            "Accuracy                        0.610723                 0.652681   \n",
            "F1                              0.723051                 0.753719   \n",
            "Recall                          0.828897                  0.86692   \n",
            "Specificity                      0.26506                 0.313253   \n",
            "Confusion Matrix  [[88, 244], [90, 436]]  [[104, 228], [70, 456]]   \n",
            "\n",
            "                                       KNN                 XGBoost  \\\n",
            "Accuracy                          0.670163                0.652681   \n",
            "F1                                0.749336                0.766458   \n",
            "Recall                            0.804183                0.929658   \n",
            "Specificity                       0.457831                0.213855   \n",
            "Confusion Matrix  [[152, 180], [103, 423]]  [[71, 261], [37, 489]]   \n",
            "\n",
            "                                     LSTM                       CNN  \n",
            "Accuracy                         0.581585                  0.610723  \n",
            "F1                                0.55181                  0.702847  \n",
            "Recall                           0.420152                  0.750951  \n",
            "Specificity                      0.837349                  0.388554  \n",
            "Confusion Matrix  [[278, 54], [305, 221]]  [[129, 203], [131, 395]]  \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}